# -*- coding: utf-8 -*-
"""Denoise with GL functional.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qdeBDkbX1v0GKFnEaNbapFVxbaELNm9f
"""

# # install conda
# !wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
# !chmod +x Miniconda3-latest-Linux-x86_64.sh
# !bash Miniconda3-latest-Linux-x86_64.sh -b -p /usr/local -f

# # install astra
# !conda install -y astra-toolbox python=3.9 -c astra-toolbox
# import sys
# sys.path.append('/usr/local/lib/python3.9/site-packages')


import numpy as np
import matplotlib.pyplot as plt
from skimage.data import shepp_logan_phantom, binary_blobs
from skimage.draw import disk
from scipy import sparse
# import astra
# import foam_ct_phantom



################ generate phantom ################
def getPhantom(nx, r = 0.9):
    """
    Define phantom image.
    
    input:
        nx - dimension of the input image
        r - radius of phantom, optional (default = 0.9)
    
    output:
        u - phantom image as 1d array of length nx*nx
    """
    # mask
    mask = np.zeros((nx,nx))
    #### disk: generate grid data, disk(center, radius, shape)
    ii,jj = disk((nx//2,nx//2),r * (nx//2))
    mask[ii,jj] = 1
    
    # binary blobs
    #### binary_blobs: in the Scikit package, blob_size_fraction: Typical linear size of blob, as a fraction of length
    u = np.float64(binary_blobs(length=nx, blob_size_fraction=0.5))
    u *= mask
    
    # return
    return u


#################### objective function ####################
## the graph Laplacian matrix (sparse)
def getAdjacency_Sparse(nx):

    num_pixels = nx**2

    # two types of indices
    row_index1 = np.arange(0, num_pixels-1)
    col_index1 = np.arange(1, num_pixels)

    row_index2 = np.arange(0, num_pixels-nx)
    col_index2 = np.arange(nx, num_pixels)

    # weights
    weight1 = np.ones(row_index1.shape[0])
    weight2 = np.ones(row_index2.shape[0])
    for r in row_index1:
      if (r+1)%nx ==0 :
        weight1[r] = 0

    # create sparse matrix
    rows = np.concatenate((row_index1, row_index2, col_index1, col_index2))
    cols = np.concatenate((col_index1, col_index2, row_index1, row_index2))
    weights = np.concatenate((weight1, weight2, weight1, weight2))

    sparse_matrix = sparse.csc_matrix((weights, (rows, cols)), shape = (num_pixels, num_pixels))

    return sparse_matrix


def getLaplacian_Sparse(nx):

    Adj = getAdjacency_Sparse(nx)
    deg = sparse.diags(Adj.sum(axis=1).A1)
    Lap = deg - Adj

    return Lap


## the gradient of the double-well function W(x)=x^2*(x-1)^2
def getWGradient(u):

    out = 2*u*(2*u-1)*(u-1)

    return out


## the gradient of the objective function
def objective_function_gradient(u, v, L, alpha, epsilon):

    out = 1/alpha * (u.ravel() - v.ravel()) + L @ u.ravel() + 1/epsilon * getWGradient(u.ravel())

    return out



#################### gradient descent #################### 
def gradient_descent(func, args, initial_vector, learning_rate, tolerance, n_iter=20000):
    u_k = initial_vector
    
    for iter in range(n_iter):
        diff = learning_rate * func(u_k, *args)
        u_k -= diff
        if np.all(np.abs(diff) <= tolerance):
          print('Early stop at iteration ', iter)
          break
        
        if iter % 40 == 0:
          r = np.sqrt(np.square(u_k-u.ravel()).mean())
          print('Iteration: ', iter, ' RMSE: ', r)

    return u_k


def gradient_descent_tuning(func, args, initial_vector, learning_rate, tolerance, n_iter=30000):
    u_k = initial_vector
    
    for iter in range(n_iter):
        diff = learning_rate * func(u_k, *args)
        u_k -= diff
        if np.all(np.abs(diff) <= tolerance):
          print('Early stop at iteration ', iter)
          break

    rmse = np.sqrt(np.square(u_k-u.ravel()).mean())
    print('>>>Iteration: ', iter, ' RMSE: ', rmse)
    return u_k


#################### generating image ####################
# np.random.seed(19)

# image size on x-axis
nx = 128
# noise level
sigma = 1

# ground truth
u = getPhantom(nx)
# noisy image
v = u + sigma * np.random.rand(u.shape[0], u.shape[1])
# graph Laplacian matrix
L = getLaplacian_Sparse(nx)
# gradient of double-well function
W = getWGradient(u.ravel())

# original root mean square error
rmse = np.sqrt(np.square(u-v).mean())
print('Root mean square error is ', rmse)


#################### tuning alpha & epsilon ####################

rmse = np.sqrt(np.square(u-v).mean())
print('rmse is ', rmse)

result = np.array([])

epsilon_range = [0.5,0.3,0.1,0.05,0.04]
alpha_range = [0.5,0.45,0.42,0.4,0.3,0.25]

for epsilon in epsilon_range:
    for alpha in alpha_range:
        x = np.zeros(nx**2)
        print('parameters: alpha is ', alpha, 'epsilon is ', epsilon)
        out = gradient_descent_tuning(objective_function_gradient, [v, L, alpha, epsilon], x, learning_rate, tolerance)
        result = np.append(result, out)

result = result.reshape([len(epsilon_range)*len(alpha_range), out.shape[0]])


#################### image denoise ####################
alpha=0.3
epsilon=0.05
tolerance = 1e-06
learning_rate=0.01
x = np.zeros(nx**2)
out = gradient_descent(objective_function_gradient, [v,L,alpha,epsilon], x, learning_rate, tolerance)



#################### showing result ####################
x_image = out.reshape(v.shape[0],v.shape[1])
fig, ax = plt.subplots(1,3)
ax[0].imshow(u)
ax[0].set_title('ground truth')
ax[1].imshow(v)
ax[1].set_title('noisy image')
ax[2].imshow(x_image)
ax[2].set_title('recovered image')
fig.tight_layout()
fig.set_figwidth(15)

